---
title: "Tuning persistent homological hyperparameters"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tuning persistent homological hyperparameters}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{tidymodels,ranger}
---

```{r options, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette will illustrate the tuning of hyperparameters at two pre-processing steps provided by {tdarec}: the computation of persistent homology from data, and the vectorization of the resulting persistence data.
Because the logic and syntax can become confusing, the vignette will also illustrate how to tune model hyperparameters in the same workflow as those of the pre-processing recipe.

```{r setup}
library(tidymodels)
library(tdarec)
```

## cubical persistent homology

The persistent homology (PH) of a real-valued function on a manifold is determined by the _sublevel set filtration_:
Given $f : X \to \mathbb{R}$, for each value $r \in \mathbb{R}$ define $X_r = f^{-1}(-\infty,r]$, so that $X_r \subseteq X_s$ whenever $r < s$.
Then the induced homomorphisms $H_k(X_r) \to H_k(X_s)$ between (singular) homology groups form a _persistence module_ whose decomposition obtains the birth--death pairs that comprise the $k^\text{th}$ persistence diagram of $X$.

While some such computations can be done analytically, most data come in the form of discrete measurements rather than functional relationships.
For example, the topographic data for the Maunga Whau caldera take the form of a 2-dimensional numeric array, with each cell containing the estimated elevation of a 100--square meter area:

```{r volcano, fig.height=5, fig.width=7}
filled.contour(volcano, color.palette = terrain.colors, asp = 1)
```

This is a discretization of a height function from latitude and longitude ($\mathbb{S}^2$, approximated locally by $\mathbb{R}^2$) to elevation ($\mathbb{R}$), and cubical persistent homology is an alternative to the simplicial computation designed specifically for data in this form.
The {ripserr} package ports the Cubical Ripser algorithm to R and serves as the engine for this pre-processing step.

## handwritten digits data

The [MNIST handwritten digits data set](https://en.wikipedia.org/wiki/MNIST_database) comprises 70,000 $28 \times 28$ pixellated black-and-white images of the numerals 0--9 obtained from forms completed by US Census Bureau field staff and Maryland high school students.
The images are partitioned into a training set of 60,000 and a testing set of 10,000, and {tdavec} comes with a 1% random sample from each.
For example, here are the first six digits, labeled $`r paste0(mnist_train$label[seq(6)], collapse = ", ")`$:

```{r mnist datum, fig.height=5, fig.width=7}
par(mfrow = c(2, 3), mar = c(.5, .5, .5, .5))
for (r in seq(6)) {
  image(
    mnist_train$digit[[r]],
    col = grey.colors(256, start = 0, end = 1, rev = TRUE),
    xaxt = "n", yaxt = "n", asp = 1
  )
}
par(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1)
```

(That the digits are rotated from their usual orientation is not a problem for the methods used here.)
To respect Tidymodels conventions, we convert the `label` column from integer to factor before continuing.

```{r mnist data}
mnist_train$label <- factor(mnist_train$label, sort(unique(mnist_train$label)))
mnist_test$label <- factor(mnist_test$label, sort(unique(mnist_test$label)))
```

## preparation

Following a standard ML approach, we prepare the training set for 6-fold cross-validation, which will be used to choose hyperparameter settings that maximize the accuracy of a classifier.
The optimized settings will be used to classify the digits in the testing set, and a comparison with the true labels will provide an estimate of the accuracy of the resulting model.

```{r data partition}
set.seed(143597)
(mnist_folds <- vfold_cv(mnist_train, v = 6))
```

For reference, we check the range of the values that populate the arrays:

```{r value range}
print(range(unlist(mnist_train$digit)))
```

This indicates that the greyscale images are coded as integers between $0$ and $2^8 - 1$, and we will use this information to inform our pre-processing steps.

# a fixed-parameter walkthrough

Before taking on the challenge of tuning, we walk through the modeling process using fixed parameters. This will help ensure that our approach is sound, for example by prompting us to think through our choices and flagging unforeseen problems.
It will also allow us to compare the performance of the model under "trialed" versus tuned parameter settings.

## pre-processing recipe

The first step is to prepare the pre-processing recipe.
Ours goes in three steps: Blur the original images, compute their cubical persistence diagrams, and transform these diagrams into vectorized persistence landscapes.
We comment on each step:

1. One limitation of cubical persistent homology is that it is sensitive only to the height difference between cells in the array, not the distances between them; see [Kaji, Sudo, & Ahara (2020)](http://arxiv.org/abs/2005.12692) for a detailed discussion. So, on topographic data, a steep peak and a gentle mound of the same height will be encoded as the same feature.
This is likely to be a problem in the classification of digits, as for example the numerals $0$ and $6$ have one "hole" each so are indistinguishable from a purely topological perspective. We can make the PH sensitive to distance by first convolving the image with a small bivariate Gaussian distribution. This _blur_ reduces the height difference between nearby positions but retains that between distant positions. For this untuned model, we allow `step_blur()` to compute a default standard deviation for the Gaussian as described in `help(blur)`.
2. Cubical persistent homology is a parameter-free computation, so no tuning is required or even possible. (With point clouds, we might care about the highest-degree features being computed, but for our height functions on $\mathbb{R}^2$ the upper bound for non-zero homology is $2 - 1 = 1$.)
3. {TDAvec} provides over a dozen vectorization methods for persistence diagrams, and each carries strengths and weaknesses. Our choice to use persistence landscapes in this vignette is somewhat arbitrary, but the nuance of the step allows us to illustrate optimization using two hyperparameters (the homological degree of the diagram to transform and the number of landscape levels to vectorize) and to point out some adjustable default behavior. We use our knowledge of the value range to specify a grid.

Only the final pre-processing step yields new numeric columns that can be used as predictors in a conventional model.
For this reason, the PH step by default assigns the placeholder role `"persistence diagram"` to its output, so that the new `digit_phom` column will not accidentally be used as a predictor.
In contrast, the blur step modifies its column `digit` in place, and [the modified column inherits the role(s) of the original](https://recipes.tidymodels.org/articles/Roles.html#role-inheritance).
This requires us to update the role of `digit`, but this can be done before or after applying the blur.

```{r persistent homological recipe}
recipe(mnist_train) |> 
  update_role(label, new_role = "outcome") |> 
  update_role(digit, new_role = "image") |> 
  step_blur(digit) |>
  step_phom_lattice(digit) |> 
  step_vpd_persistence_landscape(
    digit_phom,
    hom_degree = 1,
    xmin = 0, xmax = 255, xby = 16,
    num_levels = 3
  ) |> 
  print() -> mnist_rec
```

We can check directly whether our role assignments obtained as expected:

```{r recipe summary}
mnist_rec |> 
  prep() |> 
  print() -> mnist_prep
summary(mnist_prep)
```

And we can inspect the result of applying the prepared recipe to the training data:

```{r trained recipe}
mnist_rec |> 
  prep() |> 
  bake(new_data = mnist_train)
```

## classification forest

In the next code chunk we specify a random forest (RF) model to classify the digits using vectorized persistence landscapes.
RFs are ubiquitous in ML for their combination of flexibility and performance: They can be used almost anywhere and generally produce more models that are competitive with or superior to classical models like decision trees and generalized linear models.
They also come with a medley of tuning parameters. In this vignette, we demonstrate the use of 3 hyperparameters, each treated differently:

* The number `trees` of trees in each forest is fixed at 300.
* The minimum node size will be tuned across its default range, though here it is fixed at 6.
* The number of predictors selected for each tree will also be tuned, but the maximum value must be determined from the data. Here it is set to 2.

The {randomForest} engine is strict in its requirements: If all predictors have zero variance, i.e. there is no way to generate different predictors for different cases, then the model fails with an error.
This is a serious possibility for our setting, for instance if no degree-1 features are detected so that the persistence landscapes are uniformly zero.
To prevent this from derailing our workflow, we use the {ranger} engine, which is tolerant of this situation, instead.

```{r random forest specification}
rand_forest(
  trees = 300,
  min_n = 6,
  mtry = 2
) |> 
  set_mode("classification") |> 
  set_engine("ranger") |> 
  print() -> mnist_spec
```

## fitting and evaluation

We fit the model to the pre-processed training data:

```{r model fit}
set.seed(944327)
fit(
  mnist_spec,
  mnist_rec |> prep() |> formula(),
  data = mnist_rec |> prep() |> bake(new_data = mnist_train)
) |> 
  print() -> mnist_fit
```

Note that the model, while not optimized for accuracy on the training set, is informed by it through the preparation process, in particular the default choice of blur parameter:

```{r blur value}
mnist_prep$steps[[1]]$blur_sigmas
```

To evaluate the model, we generate predictions for the testing set and compare them to the true labels.
Because accuracy is a coarse metric for a 10-value classification task, we examine the confusion matrix to get a sense of what errors are most common.

```{r evaluate fitted model}
mnist_fit |> 
  predict(new_data = bake(prep(mnist_rec), new_data = mnist_test)) |> 
  bind_cols(select(mnist_test, label)) |> 
  print() -> mnist_pred
mnist_pred |> 
  conf_mat(truth = label, estimate = .pred_class)
mnist_pred |> 
  accuracy(truth = label, estimate = .pred_class)
```

The accuracy is poor but clearly better than chance. The confusion matrix suggests some reasons for the misclassifications, in that the most commonly confused digits are _not_ those with the same degree-1 homology; they are more geometrically than topologically similar.
This could be due to excessive blur; the standard deviation of `r mnist_prep$steps[[1]]$blur_sigmas` may be too high. We can compare this default to the value obtained through tuning.

# tuning hyperparameters

We now extend the process above to choose, bound, and tune several pre-processing and modeling hyperparameters.

## tunable pre-processing recipe

We rewrite the recipe to prepare the hyperparameters for tuning rather than to assign them fixed values.
Each parameter is given a character ID that will refer to it in the various system messages and outputs.
Beware that this section of the vignette overwrites all `mnist_*` variable names used in the previous section! This is for readability but can cause problems if parts are executed out of order.

```{r tunable persistent homological recipe}
recipe(mnist_train) |> 
  update_role(label, new_role = "outcome") |> 
  update_role(digit, new_role = "image") |> 
  step_blur(digit, blur_sigmas = tune("blur_sd")) |>
  step_phom_lattice(digit) |> 
  step_vpd_persistence_landscape(
    digit_phom,
    hom_degree = tune("pl_deg"),
    xmin = 0, xmax = 255, xby = 16,
    num_levels = tune("pl_lev")
  ) |> 
  print() -> mnist_rec
```

This recipe has three tunable parameters, as we can verify by extracting their dials:

```{r tunable recipe hyperparameters}
( rec_dials <- extract_parameter_set_dials(mnist_rec) )
```

Note that all three require finalization; like the number of randomly sampled predictors for each tree in a random forest, their ranges should not be guessed but must be determined from the content of the data.
In fact, the persistence landscape hyperparameters must be determined from columns derived by the first two steps from the input columns, not from the input columns themselves.
For this reason only, we train the recipe with some intuitive values in order to obtain these derived columns for tuning purposes:

```{r trained tunable recipe}
mnist_rec |> 
  finalize_recipe(parameters = list(blur_sd = 8, pl_deg = 0, pl_lev = 3)) |> 
  prep() |> 
  bake(new_data = mnist_train) ->
  mnist_bake
print(mnist_bake)
```

We use the input `digit` column to finalize the range of blurs and the derived `digit_phom` column to finalize the range of homological degree.
We expect the important topological features of the digits number at most 2 per image, so we manually prescribe a narrow range for `num_levels`, though it could also be learned from the training set.
Each finalized range is printed for reference:

```{r finalize recipe tuners}
( blur_sd_fin <- finalize(blur_sigmas(), mnist_train |> select(digit)) )
( pl_deg_fin <- finalize(hom_degree(), mnist_bake |> select(digit_phom)) )
( pl_lev_fin <- num_levels(range = c(1, 3)) )
```

Note that the homological degree ranges only from $0$ to $1$ because the image has dimension 2.

## tunable classification forest

As noted earlier, the three hyperparameters of the RF specification will be treated in different ways: `trees` fixed at $300$, `min_n` tuned over a default grid, and `mtry` tuned over a grid learned from the training set.

```{r tunable random forest specification}
rand_forest(
  trees = 300,
  min_n = tune("rf_node"),
  mtry = tune("rf_pred")
) |> 
  set_mode("classification") |> 
  set_engine("ranger") |> 
  print() -> mnist_spec
```

We can again check for the two tunable hyperparameters by extracting their dials:

```{r tunable model parameters}
( spec_dials <- extract_parameter_set_dials(mnist_spec) )
```

As seen in the printed dials, one hyperparameter range must be finalized.
The process for doing so is the same as for the recipe, but in this case the variables of interest are the vectorized features.
We see from the pre-processed data that these features use a consistent naming convention, and we use this convention to select them for learning the range:

```{r finalize model tuners}
( mtry_fin <- finalize(mtry(), mnist_bake |> select(contains("_pl_"))) )
```

## hyperparameter tuning

At last we arrive at the crux of this vignette!
In preparation for tuning, we wrap the pre-processing recipe and the model specification in a workflow:

```{r workflow of recipe and model}
workflow() |> 
  add_recipe(mnist_rec) |> 
  add_model(mnist_spec) |> 
  print() -> mnist_wflow
```

One way to systematically optimize the recipe and model hyperparameters---to tune the dials---is via a grid.
This can be important when the parameters have known trade-offs or the objective function is expected to have multiple optima, so the investigator wants a course-grained "map" of how performance varies across the whole parameter space.
For illustration, we construct a workflow grid by crossing a recipe grid with a model grid:

```{r prepare a tuning grid}
set.seed(319845)
mnist_rec_grid <- 
  grid_regular(blur_sd_fin, pl_deg_fin, pl_lev_fin, levels = 3) |> 
  set_names(c("blur_sd", "pl_deg", "pl_lev"))
mnist_spec_grid <- 
  grid_regular(min_n(), mtry_fin, levels = 3) |> 
  set_names(c("rf_node", "rf_pred"))
mnist_grid <- merge(mnist_rec_grid, mnist_spec_grid)
head(mnist_grid)
```

This approach is not as inefficient as it might seem:
As implemented in Tidymodels, [grid tuning recognizes the order of the pre-processing steps](https://tune.tidymodels.org/articles/extras/optimizations.html), and sometimes [of the model construction](https://www.tidymodels.org/learn/work/tune-text/#grid-search), and only performs each step once for every combination of subsequent steps, using the temporarily stored results rather than recomputing them from scratch.
The unexecuted code chunk below shows the syntax for conducting this grid search:

```{r tune workflow over a grid, eval=FALSE}
mnist_res <- tune_grid(
  mnist_wflow,
  resamples = mnist_folds,
  grid = mnist_grid,
  metrics = metric_set(accuracy, roc_auc),
  control = control_grid()
)
```

Still, however, due to the exceptionally costly computations involved, a more targeted search is preferable.
For this reason, [Motta, Tralie, &al (2019)](https://ieeexplore.ieee.org/abstract/document/8999182) recommend Bayesian optimization for ML using topological features. This procedure is also implemented in Tidymodels and executed below.
First, we combine the extracted dials from the pre-processing recipe and the model specification and update them with the finalized ranges:

```{r update parameter dials}
wflow_dials <- bind_rows(rec_dials, spec_dials) |> 
  update(blur_sd = blur_sd_fin, pl_deg = pl_deg_fin, pl_lev = pl_lev_fin) |> 
  update(rf_pred = mtry_fin)
```

The tuning syntax is similar to that of the grid search, but in place of the grid we only provide the updated dials.
For illustration, we specify 6 initial seeds with 12 tuning iterations each, and we limit the total runtime to 5 minutes.

```{r tune workflow via bayesian optimization}
set.seed(259762)
mnist_res <- tune_bayes(
  mnist_wflow,
  resamples = mnist_folds,
  param_info = wflow_dials,
  metrics = metric_set(accuracy, roc_auc),
  initial = 6, iter = 12,
  control = control_bayes(time_limit = 5)
)
```

Whereas in the previous section we fit only a single model, in this section we have obtained results for numerous hyperparameter settings and must choose among them for the final model.
We first inspect the top several models, for a sense of how rapidly performance drops away from the optimal settings, then select the top-performing settings for the final model.
We computed both accuracy and area under the ROC curve, but we use only the former to inform this choice.

```{r select hyperparameter settings}
collect_metrics(mnist_res) |> 
  filter(.metric == "accuracy") |> 
  arrange(desc(mean))
(mnist_best <- select_best(mnist_res, metric = "accuracy"))
```

## fitting and evaluation

Now, finally, we fit the tuned model to the training set:

```{r final model fit}
mnist_fin <- prep(finalize_recipe(mnist_rec, mnist_best))
set.seed(590858)
fit(
  finalize_model(mnist_spec, mnist_best),
  formula(mnist_fin),
  data = bake(mnist_fin, new_data = mnist_train)
) |> 
  print() -> mnist_fit
```

The resulting RF correctly classifies roughly half of the images---not great, but much better than the model using intuited hyperparameter values (and than pure chance).
(Note also that the `blur` standard deviation is lower than originally defaulted to.)
As before, we can look to the confusion matrix for insights into the sources of error:

```{r evaluate on training set}
mnist_fit |> 
  predict(new_data = bake(mnist_fin, new_data = mnist_train)) |> 
  bind_cols(select(mnist_train, label)) |> 
  print() -> mnist_pred
mnist_pred |> 
  conf_mat(truth = label, estimate = .pred_class)
mnist_pred |> 
  accuracy(truth = label, estimate = .pred_class)
```

Among the most-confused digits in these experiments are $1$ and $7$, $2$ and $5$, and $3$ and $5$. This is more in keeping with the topological similarities of the digits, though geometry is also clearly playing a role.

Finally, we evaluate the final model on the testing set---the holdout data from the original partition.

```{r evaluate on testing set}
mnist_fit |> 
  predict(new_data = bake(mnist_fin, new_data = mnist_test)) |> 
  bind_cols(select(mnist_test, label)) |> 
  print() -> mnist_pred
mnist_pred |> 
  conf_mat(truth = label, estimate = .pred_class)
mnist_pred |> 
  accuracy(truth = label, estimate = .pred_class)
```

Some patterns in the training set hold up, though others don't, and some frequent confusion pairings are new.
The accuracy is lower by a third than on the training set---a reminder of the importance of out-of-box evaluation---though within the 2--standard error interval of the cross-validation estimate.
While the sample is small, we can say with some confidence that topological features discriminate among them, and much more effectively after hyperparameter optimization.
